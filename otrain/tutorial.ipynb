{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcorre/otrainee/blob/Kenza/otrainee/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WKAXpAGULVD4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-691cc2a16dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#this code enables he program to function on a TPU, it needs to be added in file train.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# TPU detection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#this code enables he program to function on a TPU, it needs to be added in file train.py\n",
    "#and \n",
    "import tensorflow as tf\n",
    "try:\n",
    "    # TPU detection. \n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCaDT9slSavH"
   },
   "source": [
    "# Getting started:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OR6KAnRyI1-2"
   },
   "source": [
    "Clone the git project:\n",
    "if you want to change the name of the folder in your notebook, add the name right after the command like so: \n",
    "\n",
    "\n",
    "> !git clone https://github.com/dcorre/tbd_cnn.git name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iC9U51feZaCA"
   },
   "outputs": [],
   "source": [
    "!git clone -b Kenza https://github.com/dcorre/otrain.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m97fnUskJMRd"
   },
   "source": [
    "Install all necessary libraries: \n",
    "\n",
    "\n",
    "*   numpy\n",
    "*   matplotlib\n",
    "*   pandas\n",
    "*   shapely\n",
    "*   h5py\n",
    "*   requests\n",
    "*   scikit-learn\n",
    "*   scipy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f1bVL7GZmjd"
   },
   "outputs": [],
   "source": [
    "!pip install numpy scipy matplotlib astropy pandas shapely requests h5py scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Qyhq3rSLUbI"
   },
   "source": [
    "Since there's few complications with the newest version of tensorflow, we'll install the version 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuHq_9_zn7tK"
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install lacosmic hjson voevent-parse xmltodict astroML regions photutils keras keras-vis tensorflow cython regions  opencv-python-headless\n",
    "!python3 -m pip install --pre astroquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-Eb5j60J3DF"
   },
   "source": [
    "Move to the folder otrain (or what you called the git in the first command), and setup the environment to be able to use the executables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6i8QgbQSc2p"
   },
   "outputs": [],
   "source": [
    "cd otrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhBTZDalazzq"
   },
   "outputs": [],
   "source": [
    "!python3.7 setup.py develop --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhiAwpn61fUL"
   },
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow --yes\n",
    "!pip install tensorflow==2.3.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4OduEwBLZDf"
   },
   "source": [
    "To upload the datacube, you can either upload them manually (but they'll be deleted if you disconnect the notebook or reset it) or you can upload them from your drive with the command line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MLiWqpA4WHJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GSiA76lPo5i"
   },
   "source": [
    "# Launching the training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oDf8vhXMI4m"
   },
   "source": [
    "Running the cell below will launch the training process on the datacube provided :\n",
    "\n",
    "*   --cube : path to your cube.\n",
    "\n",
    "You can specify:\n",
    "*   the model path (--model-path) with the model name (--model-name) to store your trained model.\n",
    "*   the number of epochs (training steps), and the threshold (all candidates whose probability is greater than this value will be considered real transients: class True).\n",
    "\n",
    "It automatically prints the different evaluation metrics values (recall, precision, F1-score, Matthew correlation coefficient) and the confusion matrix, and stores the plots (ROC, precision-recall curve and the probability distribution plot), plus the folders with the misclassified candidates in the same path as your model.\n",
    "\n",
    "It also generates a datacube with the validation dataset if you want to get results with having to train it again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VOKfzL-WMAyy"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8nADHfGyz1K"
   },
   "outputs": [],
   "source": [
    "!python3.7 otrain/cli/train.py --cube ../drive/MyDrive/cube_KGuitalens1.npz --model-path cnn --model-name model --epochs 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRTtT3_aPgyR"
   },
   "source": [
    "# Getting the results of a pretrained model and generating cutouts of the misclassified candidates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2KmL-7gOurb"
   },
   "source": [
    "If you have a pre-trained model, or you want to use your trained model (from the cell above), to get results for a different threshold or a different datacube (of the same telescope), you can run the cells below: \n",
    "\n",
    "the first cell will output the metrics' values for the threshold and generates the folders FN and FP, and the second one will generated all the plots above-mentioned.\n",
    "\n",
    "You should specify:\n",
    "\n",
    "\n",
    "*   the model path, and the path to the cube you want to test it on, and the .\n",
    "*   for the first cell you'll also have to specify the threshold, and the path to where you want to store the folders \"misclassified\" of FP and FN, and \"well classified\" of 30 randomly chosen TN and 30 TP (if not specified, the default path will be the path to your model).\n",
    "\n",
    "\n",
    "but note that these outputs will be deleted if the notebook is reset.\n",
    "\n",
    "The cube used here is the validation datacube that was automatically generated by train.py, but you can apply it to a different datacube \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNFPY79_JcYb"
   },
   "outputs": [],
   "source": [
    "!python3.7 otrain/cli/diagnostic.py --path-cube-validation cnn_OAJ/CNN_training/model/validation_set/datacube/cube_validation.npz --model-path cnn/CNN_training/model/model.h5 --threshold 0.38 --path-diagnostics cnn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lO2NBO8jWoK0"
   },
   "outputs": [],
   "source": [
    "!python3.7 otrain/cli/plot_results.py --path-cube-validation cnn/CNN_training/model/validation_set/datacube/cube_validation.npz --model-path cnn/CNN_training/model/model.h5 --path-plots cnn_plots --threshold 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ehp5AFjXQGRF"
   },
   "source": [
    "The cell below will create a compressed file of the folders \"Well classified\" and \"Misclassified\" that you generated, so that you can download it and browse locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKuY06Sawu4g"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "#shutil.make_archive(outputfile, 'zip', directory)\n",
    "shutil.make_archive(\"results\", 'zip', \"cnn_results/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZDCondARcMZ"
   },
   "source": [
    "# Finding the minimum dataset size for your telescope:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xygT8CbtRlxO"
   },
   "source": [
    "We conducted a study on four different telescopes to determine the minimum dataset size, above which the performance of the model stabilizes and there's no underfitting nor overfitting. We've concluded that this value is around 6000 candidates (Real and Bogus combined), but you can test it yourself with the command below by specifying the same parameters as the training command, plus the number of sections you want to divide the dataset into. This will launch a loop on the dataset size. Beginning with one section, it adds each time a section and reinitializes the model and trained it on this subset, the following results for each size will be stored, and it generates three different plots for every results versus the dataset size in the same path as the model:\n",
    "\n",
    "*   final validation loss\n",
    "*   accuracy-validation accuracy\n",
    "*   evaluation metrics (F1-score, MCC, final validation accuracy)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwXmVmzZVQ7U"
   },
   "outputs": [],
   "source": [
    "!python3.7 otrain/cli/optimize_size.py --cube ../drive/MyDrive/cube_KGuitalens1.npz --model-path cnn_size --model-name model --epochs 15 --n_sections 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZDuyZmyZB56"
   },
   "outputs": [],
   "source": [
    "!git add otrain/train.py otrain/cli/plot_results.py otrain/plot_results.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImOaSR17FG4T"
   },
   "source": [
    "# GRAD - CAM Understanding the CNN\n",
    "In this section, we'll run the grad-cam code (gradient- class activation map) that will show us where the model focused to give a certain prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3VB-lZ5jCkN"
   },
   "outputs": [],
   "source": [
    "!python3.7 otrain/cli/grad_cam.py --cube cnn_OAJ/CNN_training/model/validation_set/datacube/cube_validation.npz --model-path cnn_OAJ/CNN_training/model/model.h5 --cam_path cnn_grad_cam --threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tL5EPHIDW9Xx"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"results\", 'zip', \"grad_cam/\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPKCilRGQaBOnd+J7nuerXo",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
